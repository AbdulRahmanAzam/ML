{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432b047c-a910-44b9-97b1-1c92f4bde7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all libraries installed successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"all libraries installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf9b335-d01b-4fbf-ae36-e25fc98c2e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wulvob get your medircations online qnb ikud v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>computer connection from cnn com wednesday es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>university degree obtain a prosperous future m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>thanks for all your answers guys i know i shou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  ounce feather bowl hummingbird opec moment ala...\n",
       "1      1  wulvob get your medircations online qnb ikud v...\n",
       "2      0   computer connection from cnn com wednesday es...\n",
       "3      1  university degree obtain a prosperous future m...\n",
       "4      0  thanks for all your answers guys i know i shou..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83448 entries, 0 to 83447\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   83448 non-null  int64 \n",
      " 1   text    83448 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of        label                                               text\n",
       "0          1  ounce feather bowl hummingbird opec moment ala...\n",
       "1          1  wulvob get your medircations online qnb ikud v...\n",
       "2          0   computer connection from cnn com wednesday es...\n",
       "3          1  university degree obtain a prosperous future m...\n",
       "4          0  thanks for all your answers guys i know i shou...\n",
       "...      ...                                                ...\n",
       "83443      0  hi given a date how do i get the last date of ...\n",
       "83444      1  now you can order software on cd or download i...\n",
       "83445      1  dear valued member canadianpharmacy provides a...\n",
       "83446      0  subscribe change profile contact us long term ...\n",
       "83447      1  get the most out of life ! viagra has helped m...\n",
       "\n",
       "[83448 rows x 2 columns]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(r'combined_data.csv')\n",
    "\n",
    "display(df.head())\n",
    "print(df.info())\n",
    "display(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28311aa1-5b75-4d52-ab0c-06f3ae6ec940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking for the empty values\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c1a3c37-e246-4933-9c2a-69ddcabdd710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\azama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\azama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\azama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\azama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      7938\n",
      "           1       0.99      0.96      0.97      8752\n",
      "\n",
      "    accuracy                           0.97     16690\n",
      "   macro avg       0.97      0.97      0.97     16690\n",
      "weighted avg       0.97      0.97      0.97     16690\n",
      "\n",
      "[[7826  112]\n",
      " [ 326 8426]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "# since already in 0 and 1\n",
    "# df['label'] = df['label'].map({'ham':0, 'spam':1})\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "clean_texts = []\n",
    "\n",
    "for text in df['text']:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [stemmer.stem(lemmatizer.lemmatize(t)) for t in tokens]\n",
    "    # removed these lines to keep it simpler and efficient\n",
    "    # tagged = pos_tag(tokens)\n",
    "    # clean_texts.append(\" \".join([w for w, pos in tagged]))\n",
    "    clean_texts.append(\" \".join(tokens))\n",
    "\n",
    "\n",
    "df['cleaned_text'] = clean_texts\n",
    "\n",
    "\n",
    "\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer() #simple bag of words\n",
    "xtrainVec = vectorizer.fit_transform(Xtrain)\n",
    "xtestVec = vectorizer.transform(Xtest)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(xtrainVec, ytrain)\n",
    "\n",
    "y_pred = model.predict(xtestVec)\n",
    "\n",
    "print(classification_report(ytest, y_pred))\n",
    "print(confusion_matrix(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb544a11-06ec-4a3b-ae63-38a4c10c4956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prediction: [0]\n",
      "Sample Probabilities: [[1.00000000e+00 4.82144509e-36]]\n",
      "Sample Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "sample = [\"thanks for all your answers guys i know i should have checked the rsync manual but i would rather get a escapenumber sure answer from one of you this is my current script bin bash rsync avt \\\\ exclude alpha \\\\ exclude arm \\\\ exclude hppa \\\\ exclude hurd \\\\ exclude iaescapenumber \\\\ exclude mescapenumberk \\\\ exclude mips \\\\ exclude mipsel \\\\ exclude multi arch \\\\ exclude powerpc \\\\ exclude sescapenumber \\\\ exclude sh \\\\ exclude sparc \\\\ exclude source \\\\ ftp de debian org debian cd var www mirror debian cd i know loads of excludes for now will include more distros soon from the rsync manual del an alias for delete during delete delete extraneous files from dest dirs delete before receiver deletes before transfer default delete during receiver deletes during xfer not before delete after receiver deletes after transfer not before delete excluded also delete excluded files from dest dirs which delete would you suggest i use thanks again john escapelong on escapenumber escapenumber escapenumber olleg samoylov wrote jonathan escapelong wrote sorry for the banal question my favourite keys for escapenumber stage rsync rsync verbose recursive links hard links times filter 'r tmp ' delete after delay updates source url destination log file olleg samoylov www escapelong org mirror escapelong org rcrack escapelong org ninux org wireless community rome \"]\n",
    "\n",
    "sampleVec = vectorizer.transform(sample)\n",
    "prediction = model.predict(sampleVec)\n",
    "print(f\"Sample Prediction: {prediction}\")\n",
    "print(f\"Sample Probabilities: {model.predict_proba(sampleVec)}\")\n",
    "print(f\"Sample Accuracy: {model.score(sampleVec, [1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bed2cc-5da7-4a8a-96c9-787c41348474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (TF GPU)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
